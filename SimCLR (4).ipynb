{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsl1Q0SLpJPG"
      },
      "source": [
        "# SimCLR Code with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGur3GO-r91s",
        "outputId": "16997cca-9a29-4c5d-fd65-3096c0a8ed85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8zUcvHv-m3z"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-BwBTZaxcg9"
      },
      "outputs": [],
      "source": [
        "#!sudo apt-get update\n",
        "#!sudo apt-get install build-essential\n",
        "#!sudo apt-get install python3-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JOU5s0qxF5J"
      },
      "outputs": [],
      "source": [
        "!pip install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tIJ6vCLpIgZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_models as tfm\n",
        "import tensorflow_datasets as tfds\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from utils.NT_Xent import NT_Xent_tf\n",
        "from utils.evaluation_metrics import get_top_k_accuracy\n",
        "from training.learning_rate_schedule import MyLRSchedule\n",
        "from training.train import train_step, train\n",
        "from utils.test import test\n",
        "from utils.data_augmentations import augment_image\n",
        "import seaborn as sns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNLBV90g_KXt"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKAvUywOoHOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a24932-0c0d-40a1-d0d4-fb5ae70cc8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHZ_P-Xw_L94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e7d9e4-d5a2-4846-f64b-d55bac171375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files to: /content/imagenet_data\n"
          ]
        }
      ],
      "source": [
        "tar_file_path = '/content/drive/MyDrive/imagenet-lsvrc-2012/ILSVRC2012_img_train_t3.tar'\n",
        "extract_path = '/content/imagenet_data'\n",
        "\n",
        "# Extract the tar file\n",
        "with tarfile.open(tar_file_path, 'r') as tar:\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(f\"Extracted files to: {extract_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JIZJioE0EBUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ff1c18-02e6-4338-aa61-b990bd1f6506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted n02094258.tar into /content/organized_imagenet_data/n02094258\n",
            "Extracted n02100583.tar into /content/organized_imagenet_data/n02100583\n",
            "Extracted n02086240.tar into /content/organized_imagenet_data/n02086240\n",
            "Extracted n02105162.tar into /content/organized_imagenet_data/n02105162\n",
            "Extracted n02109047.tar into /content/organized_imagenet_data/n02109047\n",
            "Extracted n02093647.tar into /content/organized_imagenet_data/n02093647\n",
            "Extraction and organization complete.\n"
          ]
        }
      ],
      "source": [
        "imagenet_tar_path = '/content/imagenet_data'\n",
        "output_path = '/content/organized_imagenet_data'\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "class_count = 0\n",
        "class_limit = 5\n",
        "\n",
        "# Loop through all .tar files\n",
        "for tar_file in os.listdir(imagenet_tar_path):\n",
        "    if tar_file.endswith('.tar'):\n",
        "        class_name = tar_file.split('.tar')[0]  # Extract class name (e.g., n02085620)\n",
        "        class_output_dir = os.path.join(output_path, class_name)\n",
        "\n",
        "        # Make directory for this class\n",
        "        os.makedirs(class_output_dir, exist_ok=True)\n",
        "\n",
        "        # Extract .tar file into the class directory\n",
        "        tar_file_path = os.path.join(imagenet_tar_path, tar_file)\n",
        "        with tarfile.open(tar_file_path, 'r') as tar:\n",
        "            tar.extractall(path=class_output_dir)\n",
        "\n",
        "        print(f\"Extracted {tar_file} into {class_output_dir}\")\n",
        "\n",
        "    class_count += 1\n",
        "    if class_count > class_limit:\n",
        "        break\n",
        "\n",
        "print(\"Extraction and organization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsvQHFEsyPHG"
      },
      "outputs": [],
      "source": [
        "picture_limit = 10000000\n",
        "output_path = '/content/organized_imagenet_data'\n",
        "\n",
        "for class_name in os.listdir(output_path):\n",
        "    count = 0\n",
        "    for image_name in os.listdir(os.path.join(output_path, class_name)):\n",
        "        count += 1\n",
        "        if count > picture_limit:\n",
        "            del_path = os.path.join(output_path, class_name, image_name)\n",
        "            os.remove(del_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_Y__6fBGPX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf862c8-b9b0-4c98-d967-06b7fec9c772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1041 files belonging to 6 classes.\n",
            "Classes: ['n02086240', 'n02093647', 'n02094258', 'n02100583', 'n02105162', 'n02109047']\n",
            "Image shape: (16, 224, 224, 3)\n",
            "Labels: tf.Tensor([0 5 4 1 0 1 4 3 5 1 2 2 2 2 5 2], shape=(16,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "batch_size = 16\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "# Load dataset\n",
        "dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    output_path,\n",
        "    labels='inferred',  # Automatically infer labels from subfolder names\n",
        "    label_mode='int',   # Labels are integers\n",
        "    batch_size=batch_size,\n",
        "    image_size=(img_height, img_width),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Print class names\n",
        "class_names = dataset.class_names\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Inspect a batch of images\n",
        "for images, labels in dataset.take(1):\n",
        "    print(\"Image shape:\", images.shape)\n",
        "    print(\"Labels:\", labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7CejBSFGnV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74356990-13c3-4bc5-c87b-ecf55ded4760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 4 batches\n",
            "Validation dataset size: 1 batches\n",
            "Testing dataset size: 1 batches\n"
          ]
        }
      ],
      "source": [
        "# Calculate the sizes of each split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size  # Ensure full split\n",
        "\n",
        "# Unbatch the dataset\n",
        "full_dataset = dataset.unbatch()\n",
        "\n",
        "# Create the training, validation, and testing datasets\n",
        "train_dataset = full_dataset.take(train_size).batch(batch_size)\n",
        "val_dataset = full_dataset.skip(train_size).take(val_size).batch(batch_size)\n",
        "test_dataset = full_dataset.skip(train_size + val_size).take(test_size).batch(batch_size)\n",
        "\n",
        "# Print the sizes of the resulting datasets\n",
        "train_dataset_count = sum(1 for _ in train_dataset)\n",
        "val_dataset_count = sum(1 for _ in val_dataset)\n",
        "test_dataset_count = sum(1 for _ in test_dataset)\n",
        "\n",
        "print(f\"Training dataset size: {train_dataset_count} batches\")\n",
        "print(f\"Validation dataset size: {val_dataset_count} batches\")\n",
        "print(f\"Testing dataset size: {test_dataset_count} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretraining ResNet-50 with Linear and Non-linear Projection Heads"
      ],
      "metadata": {
        "id": "pi_sFfZKTCLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### non-linear proejction head"
      ],
      "metadata": {
        "id": "SHeNDqw0TEnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the necessary variables\n",
        "epochs = 100\n",
        "temperature = 0.5\n",
        "base_learning_rate = 0.03 * batch_size / 256\n",
        "warmup_epochs = 10\n",
        "\n",
        "# Create the learning rate schedule\n",
        "lr_schedule = MyLRSchedule(base_learning_rate, warmup_epochs * train_dataset_count, epochs * train_dataset_count)\n",
        "\n",
        "# Create the optimizer with the learning rate schedule\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=lr_schedule,\n",
        "    momentum=0.9,\n",
        "    nesterov=True,\n",
        "    weight_decay=None,\n",
        ")\n",
        "\n",
        "## define mdoel (backbone + projection head)\n",
        "backbone = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(224, 224, 3),\n",
        "    pooling='avg'\n",
        ")\n",
        "backbone.trainable = True # just like this in pretraining\n",
        "\n",
        "projection_head = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(128)\n",
        "])\n",
        "projection_head.trainable = True\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "feature_extractor = backbone(inputs)\n",
        "projection_output = projection_head(feature_extractor)\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=projection_output)\n",
        "\n",
        "# train the model\n",
        "model, optimizer, losses, top_1_acuracies, top_5_acuracies, test_accuracies = train(epochs, train_dataset, model, optimizer, temperature, print_acc=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7b1b7cfTD56",
        "outputId": "61b5ad80-403d-4690-acff-1760dc263c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 2.7400\n",
            "Epoch 2, Average Loss: 2.8292\n",
            "Epoch 3, Average Loss: 2.7059\n",
            "Epoch 4, Average Loss: 2.6213\n",
            "Epoch 5, Average Loss: 2.5833\n",
            "Epoch 6, Average Loss: 2.4885\n",
            "Epoch 7, Average Loss: 2.3953\n",
            "Epoch 8, Average Loss: 2.3339\n",
            "Epoch 9, Average Loss: 2.3082\n",
            "Epoch 10, Average Loss: 2.2668\n",
            "Epoch 11, Average Loss: 2.2188\n",
            "Epoch 12, Average Loss: 2.2321\n",
            "Epoch 13, Average Loss: 2.1114\n",
            "Epoch 14, Average Loss: 2.3681\n",
            "Epoch 15, Average Loss: 2.4951\n",
            "Epoch 16, Average Loss: 2.8228\n",
            "Epoch 17, Average Loss: 2.6576\n",
            "Epoch 18, Average Loss: 2.5739\n",
            "Epoch 19, Average Loss: 2.9459\n",
            "Epoch 20, Average Loss: 2.8859\n",
            "Epoch 21, Average Loss: 2.8566\n",
            "Epoch 22, Average Loss: 2.7003\n",
            "Epoch 23, Average Loss: 2.3059\n",
            "Epoch 24, Average Loss: 2.6411\n",
            "Epoch 25, Average Loss: 2.4306\n",
            "Epoch 26, Average Loss: 2.1939\n",
            "Epoch 27, Average Loss: 2.5035\n",
            "Epoch 28, Average Loss: 2.3431\n",
            "Epoch 29, Average Loss: 2.2950\n",
            "Epoch 30, Average Loss: 2.2808\n",
            "Epoch 31, Average Loss: 2.2275\n",
            "Epoch 32, Average Loss: 2.2227\n",
            "Epoch 33, Average Loss: 2.2887\n",
            "Epoch 34, Average Loss: 2.2709\n",
            "Epoch 35, Average Loss: 2.3922\n",
            "Epoch 36, Average Loss: 2.2778\n",
            "Epoch 37, Average Loss: 2.2933\n",
            "Epoch 38, Average Loss: 2.1450\n",
            "Epoch 39, Average Loss: 2.0896\n",
            "Epoch 40, Average Loss: 2.3495\n",
            "Epoch 41, Average Loss: 2.1561\n",
            "Epoch 42, Average Loss: 2.1892\n",
            "Epoch 43, Average Loss: 2.3606\n",
            "Epoch 44, Average Loss: 2.2239\n",
            "Epoch 45, Average Loss: 2.2478\n",
            "Epoch 46, Average Loss: 2.1814\n",
            "Epoch 47, Average Loss: 2.1911\n",
            "Epoch 48, Average Loss: 2.1867\n",
            "Epoch 49, Average Loss: 2.1223\n",
            "Epoch 50, Average Loss: 2.1542\n",
            "Epoch 51, Average Loss: 1.9994\n",
            "Epoch 52, Average Loss: 2.0083\n",
            "Epoch 53, Average Loss: 2.1397\n",
            "Epoch 54, Average Loss: 2.0235\n",
            "Epoch 55, Average Loss: 1.9431\n",
            "Epoch 56, Average Loss: 1.9394\n",
            "Epoch 57, Average Loss: 2.0170\n",
            "Epoch 58, Average Loss: 1.9619\n",
            "Epoch 59, Average Loss: 2.1156\n",
            "Epoch 60, Average Loss: 2.1300\n",
            "Epoch 61, Average Loss: 1.9651\n",
            "Epoch 62, Average Loss: 2.0488\n",
            "Epoch 63, Average Loss: 1.9617\n",
            "Epoch 64, Average Loss: 1.9425\n",
            "Epoch 65, Average Loss: 1.9089\n",
            "Epoch 66, Average Loss: 1.9279\n",
            "Epoch 67, Average Loss: 1.9344\n",
            "Epoch 68, Average Loss: 1.9502\n",
            "Epoch 69, Average Loss: 2.0025\n",
            "Epoch 70, Average Loss: 1.8728\n",
            "Epoch 71, Average Loss: 1.9897\n",
            "Epoch 72, Average Loss: 2.0324\n",
            "Epoch 73, Average Loss: 1.7845\n",
            "Epoch 74, Average Loss: 1.8913\n",
            "Epoch 75, Average Loss: 1.8973\n",
            "Epoch 76, Average Loss: 1.9517\n",
            "Epoch 77, Average Loss: 1.9131\n",
            "Epoch 78, Average Loss: 1.8886\n",
            "Epoch 79, Average Loss: 1.9414\n",
            "Epoch 80, Average Loss: 1.9107\n",
            "Epoch 81, Average Loss: 1.9509\n",
            "Epoch 82, Average Loss: 1.9213\n",
            "Epoch 83, Average Loss: 1.8852\n",
            "Epoch 84, Average Loss: 1.8581\n",
            "Epoch 85, Average Loss: 1.9497\n",
            "Epoch 86, Average Loss: 1.7825\n",
            "Epoch 87, Average Loss: 1.8738\n",
            "Epoch 88, Average Loss: 1.9657\n",
            "Epoch 89, Average Loss: 1.8069\n",
            "Epoch 90, Average Loss: 1.8526\n",
            "Epoch 91, Average Loss: 1.8331\n",
            "Epoch 92, Average Loss: 1.9409\n",
            "Epoch 93, Average Loss: 1.8781\n",
            "Epoch 94, Average Loss: 2.0285\n",
            "Epoch 95, Average Loss: 1.8630\n",
            "Epoch 96, Average Loss: 1.9673\n",
            "Epoch 97, Average Loss: 1.8944\n",
            "Epoch 98, Average Loss: 1.8939\n",
            "Epoch 99, Average Loss: 1.9224\n",
            "Epoch 100, Average Loss: 1.8671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('models/pretrained_resnet_nonlinear_head.keras')"
      ],
      "metadata": {
        "id": "Q31LIaSr39lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "plt.plot(losses, label='Training Loss')\n",
        "plt.title('Training Loss for Non-linear Head')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('figures/nonlinear_pretraining_loss.png')"
      ],
      "metadata": {
        "id": "PzSeomP3dIUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### linear projection head"
      ],
      "metadata": {
        "id": "0pT-fJ7GTG3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the necessary variables\n",
        "epochs = 100\n",
        "temperature = 0.5\n",
        "base_learning_rate = 0.03 * batch_size / 256\n",
        "warmup_epochs = 10\n",
        "\n",
        "# Create the learning rate schedule\n",
        "lr_schedule = MyLRSchedule(base_learning_rate, warmup_epochs * train_dataset_count, epochs * train_dataset_count)\n",
        "\n",
        "# Create the optimizer with the learning rate schedule\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=lr_schedule,\n",
        "    momentum=0.9,\n",
        "    nesterov=True,\n",
        "    weight_decay=None,\n",
        ")\n",
        "\n",
        "## define mdoel (backbone + projection head)\n",
        "backbone = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(224, 224, 3),\n",
        "    pooling='avg'\n",
        ")\n",
        "backbone.trainable = True # just like this in pretraining\n",
        "\n",
        "projection_head = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(128)\n",
        "])\n",
        "projection_head.trainable = True\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "feature_extractor = backbone(inputs)\n",
        "projection_output = projection_head(feature_extractor)\n",
        "model_nonloienar = tf.keras.models.Model(inputs=inputs, outputs=projection_output)\n",
        "\n",
        "# train the model\n",
        "model_nonloienar, optimizer, losses_nonlinear, top_1_acuracies, top_5_acuracies, test_accuracies = train(epochs, train_dataset, model_nonloienar, optimizer, temperature, print_acc=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-02l5RzYTJHR",
        "outputId": "e58e18d2-c7b3-46e5-e22f-4fae90c540b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 2.6166\n",
            "Epoch 2, Average Loss: 2.6955\n",
            "Epoch 3, Average Loss: 2.6085\n",
            "Epoch 4, Average Loss: 2.5898\n",
            "Epoch 5, Average Loss: 2.5270\n",
            "Epoch 6, Average Loss: 2.3617\n",
            "Epoch 7, Average Loss: 2.3030\n",
            "Epoch 8, Average Loss: 2.4121\n",
            "Epoch 9, Average Loss: 2.4311\n",
            "Epoch 10, Average Loss: 2.4213\n",
            "Epoch 11, Average Loss: 2.2522\n",
            "Epoch 12, Average Loss: 2.2802\n",
            "Epoch 13, Average Loss: 2.2557\n",
            "Epoch 14, Average Loss: 2.3357\n",
            "Epoch 15, Average Loss: 2.2401\n",
            "Epoch 16, Average Loss: 2.4531\n",
            "Epoch 17, Average Loss: 2.3928\n",
            "Epoch 18, Average Loss: 2.3849\n",
            "Epoch 19, Average Loss: 2.1766\n",
            "Epoch 20, Average Loss: 2.0193\n",
            "Epoch 21, Average Loss: 2.1311\n",
            "Epoch 22, Average Loss: 2.1899\n",
            "Epoch 23, Average Loss: 2.6304\n",
            "Epoch 24, Average Loss: 2.6397\n",
            "Epoch 25, Average Loss: 2.3502\n",
            "Epoch 26, Average Loss: 2.5126\n",
            "Epoch 27, Average Loss: 2.2565\n",
            "Epoch 28, Average Loss: 2.3170\n",
            "Epoch 29, Average Loss: 2.2543\n",
            "Epoch 30, Average Loss: 2.1492\n",
            "Epoch 31, Average Loss: 2.1098\n",
            "Epoch 32, Average Loss: 2.1086\n",
            "Epoch 33, Average Loss: 2.0375\n",
            "Epoch 34, Average Loss: 2.0339\n",
            "Epoch 35, Average Loss: 2.0888\n",
            "Epoch 36, Average Loss: 2.0040\n",
            "Epoch 37, Average Loss: 2.0789\n",
            "Epoch 38, Average Loss: 2.2343\n",
            "Epoch 39, Average Loss: 1.9719\n",
            "Epoch 40, Average Loss: 2.0903\n",
            "Epoch 41, Average Loss: 2.0166\n",
            "Epoch 42, Average Loss: 1.9808\n",
            "Epoch 43, Average Loss: 2.0756\n",
            "Epoch 44, Average Loss: 2.0453\n",
            "Epoch 45, Average Loss: 2.1283\n",
            "Epoch 46, Average Loss: 2.0621\n",
            "Epoch 47, Average Loss: 1.9744\n",
            "Epoch 48, Average Loss: 2.0669\n",
            "Epoch 49, Average Loss: 2.0024\n",
            "Epoch 50, Average Loss: 1.8654\n",
            "Epoch 51, Average Loss: 1.9449\n",
            "Epoch 52, Average Loss: 1.9106\n",
            "Epoch 53, Average Loss: 1.9081\n",
            "Epoch 54, Average Loss: 2.0043\n",
            "Epoch 55, Average Loss: 1.9242\n",
            "Epoch 56, Average Loss: 1.9536\n",
            "Epoch 57, Average Loss: 1.8146\n",
            "Epoch 58, Average Loss: 1.8542\n",
            "Epoch 59, Average Loss: 1.9488\n",
            "Epoch 60, Average Loss: 1.7410\n",
            "Epoch 61, Average Loss: 1.9540\n",
            "Epoch 62, Average Loss: 1.8253\n",
            "Epoch 63, Average Loss: 1.8435\n",
            "Epoch 64, Average Loss: 1.8788\n",
            "Epoch 65, Average Loss: 1.9023\n",
            "Epoch 66, Average Loss: 1.9301\n",
            "Epoch 67, Average Loss: 1.9609\n",
            "Epoch 68, Average Loss: 1.8560\n",
            "Epoch 69, Average Loss: 1.8473\n",
            "Epoch 70, Average Loss: 1.9443\n",
            "Epoch 71, Average Loss: 1.7888\n",
            "Epoch 72, Average Loss: 1.8950\n",
            "Epoch 73, Average Loss: 1.9416\n",
            "Epoch 74, Average Loss: 1.9182\n",
            "Epoch 75, Average Loss: 1.8084\n",
            "Epoch 76, Average Loss: 1.8508\n",
            "Epoch 77, Average Loss: 1.7868\n",
            "Epoch 78, Average Loss: 1.7478\n",
            "Epoch 79, Average Loss: 1.8769\n",
            "Epoch 80, Average Loss: 1.7284\n",
            "Epoch 81, Average Loss: 1.8094\n",
            "Epoch 82, Average Loss: 1.7816\n",
            "Epoch 83, Average Loss: 1.7780\n",
            "Epoch 84, Average Loss: 1.7569\n",
            "Epoch 85, Average Loss: 1.7764\n",
            "Epoch 86, Average Loss: 1.7561\n",
            "Epoch 87, Average Loss: 1.8601\n",
            "Epoch 88, Average Loss: 1.7331\n",
            "Epoch 89, Average Loss: 1.7477\n",
            "Epoch 90, Average Loss: 1.7853\n",
            "Epoch 91, Average Loss: 1.8647\n",
            "Epoch 92, Average Loss: 1.8795\n",
            "Epoch 93, Average Loss: 1.7807\n",
            "Epoch 94, Average Loss: 1.8468\n",
            "Epoch 95, Average Loss: 1.7149\n",
            "Epoch 96, Average Loss: 1.8343\n",
            "Epoch 97, Average Loss: 1.7532\n",
            "Epoch 98, Average Loss: 1.9019\n",
            "Epoch 99, Average Loss: 1.8514\n",
            "Epoch 100, Average Loss: 1.8760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_nonloienar.save('models/pretrained_resnet_linear_head.keras')"
      ],
      "metadata": {
        "id": "inziqTPpBz-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf() ## to empty the buffer of plt - it showed more results every time I ran the code\n",
        "plt.plot(losses_nonlinear, label='Training Loss')\n",
        "plt.title('Training Loss for Linear Projection Head')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('figures/linear_pretraining_loss.png')"
      ],
      "metadata": {
        "id": "HIKMwdTUBzzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### loss comparison"
      ],
      "metadata": {
        "id": "TkNRYX6kB0SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "plt.plot(losses, label='Non-linear projection head')\n",
        "plt.plot(losses_nonlinear, label='Linear projection head')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('figures/loss_comparison.png')"
      ],
      "metadata": {
        "id": "h3ozAEwKB35B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Classifier Evaluation Setting + Sample Run"
      ],
      "metadata": {
        "id": "9o1pl6ZQgm3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load saved model from keras\n",
        "nonlinear_model_path = '/content/drive/MyDrive/ecbm4040-finalproject/models/pretrained_resnet_nonlinear_head.keras'\n",
        "resnet_pretrained_nonlinear_head = tf.keras.models.load_model(nonlinear_model_path)\n",
        "\n",
        "epochs = 15\n",
        "temperature = 0.7\n",
        "base_learning_rate = 0.1 * batch_size / 256\n",
        "warmup_epochs = 0\n",
        "n_classes = 6\n",
        "\n",
        "# Create the optimizer with the learning rate schedule\n",
        "optimizer = tf.keras.optimizers.SGD(\n",
        "    learning_rate=base_learning_rate,\n",
        "    momentum=0.9,\n",
        "    nesterov=True,\n",
        "    weight_decay=None,\n",
        ")\n",
        "\n",
        "# Create the loss function or criterion\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Add linear classifier on top the backbone\n",
        "linear_classifier = tf.keras.models.Sequential([tf.keras.layers.Dense(n_classes, activation=None)])\n",
        "linear_classifier.trainable = True\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "feature_extractor = resnet_pretrained_nonlinear_head(inputs)\n",
        "linear_classifier_output = linear_classifier(feature_extractor)\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=linear_classifier_output)\n",
        "\n",
        "## compile and run model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "model.fit(train_dataset, epochs=epochs)"
      ],
      "metadata": {
        "id": "QT8SRmISO9Bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b181e4-f811-4883-b6c7-047201d78803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 6s/step - accuracy: 0.2928 - loss: 1.8764\n",
            "Epoch 2/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.6705 - loss: 1.0129\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8272 - loss: 0.4705\n",
            "Epoch 4/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8210 - loss: 0.5299\n",
            "Epoch 5/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8982 - loss: 0.4535\n",
            "Epoch 6/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7973 - loss: 0.7167\n",
            "Epoch 7/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7806 - loss: 0.6331\n",
            "Epoch 8/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8216 - loss: 0.7157\n",
            "Epoch 9/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8899 - loss: 0.4174\n",
            "Epoch 10/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8079 - loss: 0.6831\n",
            "Epoch 11/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8127 - loss: 0.7445\n",
            "Epoch 12/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8614 - loss: 0.4207\n",
            "Epoch 13/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9018 - loss: 0.5019\n",
            "Epoch 14/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8196 - loss: 0.5259\n",
            "Epoch 15/15\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8280 - loss: 0.7190\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79220c1a2b90>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get loss and accuracy over test dataset\n",
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xpdp6EgdPy4q",
        "outputId": "4921177d-adb2-45f2-db7b-5e8e6ceaf9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3750 - loss: 6.3851\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.385066509246826, 0.375]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "plt.plot(model.history.history['loss'], label='Linear Classifier Loss')\n",
        "plt.title('Linear Classifier Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.savefig('figures/linear_classifier_sample_running_loss.png')"
      ],
      "metadata": {
        "id": "NTIgDBf7NM_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "plt.plot(model.history.history['accuracy'], label='Linear Classifier Accuracy')\n",
        "plt.title('Linear Classifier Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('figures/linear_classifier_sample_running_accuracy.png')"
      ],
      "metadata": {
        "id": "p6Liuy3lB3oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('models/linear_classifier_sample_run.keras')"
      ],
      "metadata": {
        "id": "xM3BAOvYPX94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 1: Augmentation Pair testing\n",
        "\n",
        "Here I run the first experiment for finding the best combo of the data augmentation functions. Since the table in the original paper (figure 5) is not symmetric, I went over the ['aug1', 'aug2'] combinations two times"
      ],
      "metadata": {
        "id": "7SuVZ1Yc82pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = ['random_crop_and_resize', 'horizontal_flip', 'color_distortion', 'gaussian_blur', 'cutout', 'sobel_filter', 'gaussian_noise', 'rotate']\n",
        "\n",
        "## get all the augmentaion pairs\n",
        "## since\n",
        "all_augmentation_pairs = []\n",
        "\n",
        "for aug in augmentations:\n",
        "    for aug2 in augmentations:\n",
        "        all_augmentation_pairs.append((aug, aug2))"
      ],
      "metadata": {
        "id": "3vhC2Bz7OhJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "temperature = 0.7\n",
        "base_learning_rate = 0.1 * batch_size / 256\n",
        "warmup_epochs = 0\n",
        "n_classes = 6\n",
        "nonlinear_model_path = '/content/drive/MyDrive/ecbm4040-finalproject/models/pretrained_resnet_nonlinear_head.keras'\n",
        "\n",
        "results = {pair_of_augmentations: [0.0, 0.0] for pair_of_augmentations in all_augmentation_pairs}\n",
        "\n",
        "for augmentation_pair in all_augmentation_pairs:\n",
        "    print(f\"Augmentation pair: {augmentation_pair}\")\n",
        "\n",
        "    ### linear classifier evaluation here\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=base_learning_rate, momentum=0.9, nesterov=True, weight_decay=None)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    resnet_pretrained_nonlinear_head = tf.keras.models.load_model(nonlinear_model_path)\n",
        "    resnet_pretrained_nonlinear_head.trainable = False\n",
        "\n",
        "    linear_classifier = tf.keras.models.Sequential([tf.keras.layers.Dense(n_classes, activation=None)])\n",
        "    linear_classifier.trainable = True\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "    feature_extractor = resnet_pretrained_nonlinear_head(inputs)\n",
        "    linear_classifier_output = linear_classifier(feature_extractor)\n",
        "    model = tf.keras.models.Model(inputs=inputs, outputs=linear_classifier_output)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "    model.fit(train_dataset, epochs=epochs, verbose=0)\n",
        "\n",
        "    results[augmentation_pair][0] = model.history.history['accuracy'][-1] #get train accuracy\n",
        "    results[augmentation_pair][1] = model.evaluate(test_dataset, verbose=0)[1] #get test accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUd6f_Un70oA",
        "outputId": "666410e3-37f7-4f72-f08e-c7e1bd0b222f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation pair: ('random_crop_and_resize', 'random_crop_and_resize')\n",
            "Augmentation pair: ('random_crop_and_resize', 'horizontal_flip')\n",
            "Augmentation pair: ('random_crop_and_resize', 'color_distortion')\n",
            "Augmentation pair: ('random_crop_and_resize', 'gaussian_blur')\n",
            "Augmentation pair: ('random_crop_and_resize', 'cutout')\n",
            "Augmentation pair: ('random_crop_and_resize', 'sobel_filter')\n",
            "Augmentation pair: ('random_crop_and_resize', 'gaussian_noise')\n",
            "Augmentation pair: ('random_crop_and_resize', 'rotate')\n",
            "Augmentation pair: ('horizontal_flip', 'random_crop_and_resize')\n",
            "Augmentation pair: ('horizontal_flip', 'horizontal_flip')\n",
            "Augmentation pair: ('horizontal_flip', 'color_distortion')\n",
            "Augmentation pair: ('horizontal_flip', 'gaussian_blur')\n",
            "Augmentation pair: ('horizontal_flip', 'cutout')\n",
            "Augmentation pair: ('horizontal_flip', 'sobel_filter')\n",
            "Augmentation pair: ('horizontal_flip', 'gaussian_noise')\n",
            "Augmentation pair: ('horizontal_flip', 'rotate')\n",
            "Augmentation pair: ('color_distortion', 'random_crop_and_resize')\n",
            "Augmentation pair: ('color_distortion', 'horizontal_flip')\n",
            "Augmentation pair: ('color_distortion', 'color_distortion')\n",
            "Augmentation pair: ('color_distortion', 'gaussian_blur')\n",
            "Augmentation pair: ('color_distortion', 'cutout')\n",
            "Augmentation pair: ('color_distortion', 'sobel_filter')\n",
            "Augmentation pair: ('color_distortion', 'gaussian_noise')\n",
            "Augmentation pair: ('color_distortion', 'rotate')\n",
            "Augmentation pair: ('gaussian_blur', 'random_crop_and_resize')\n",
            "Augmentation pair: ('gaussian_blur', 'horizontal_flip')\n",
            "Augmentation pair: ('gaussian_blur', 'color_distortion')\n",
            "Augmentation pair: ('gaussian_blur', 'gaussian_blur')\n",
            "Augmentation pair: ('gaussian_blur', 'cutout')\n",
            "Augmentation pair: ('gaussian_blur', 'sobel_filter')\n",
            "Augmentation pair: ('gaussian_blur', 'gaussian_noise')\n",
            "Augmentation pair: ('gaussian_blur', 'rotate')\n",
            "Augmentation pair: ('cutout', 'random_crop_and_resize')\n",
            "Augmentation pair: ('cutout', 'horizontal_flip')\n",
            "Augmentation pair: ('cutout', 'color_distortion')\n",
            "Augmentation pair: ('cutout', 'gaussian_blur')\n",
            "Augmentation pair: ('cutout', 'cutout')\n",
            "Augmentation pair: ('cutout', 'sobel_filter')\n",
            "Augmentation pair: ('cutout', 'gaussian_noise')\n",
            "Augmentation pair: ('cutout', 'rotate')\n",
            "Augmentation pair: ('sobel_filter', 'random_crop_and_resize')\n",
            "Augmentation pair: ('sobel_filter', 'horizontal_flip')\n",
            "Augmentation pair: ('sobel_filter', 'color_distortion')\n",
            "Augmentation pair: ('sobel_filter', 'gaussian_blur')\n",
            "Augmentation pair: ('sobel_filter', 'cutout')\n",
            "Augmentation pair: ('sobel_filter', 'sobel_filter')\n",
            "Augmentation pair: ('sobel_filter', 'gaussian_noise')\n",
            "Augmentation pair: ('sobel_filter', 'rotate')\n",
            "Augmentation pair: ('gaussian_noise', 'random_crop_and_resize')\n",
            "Augmentation pair: ('gaussian_noise', 'horizontal_flip')\n",
            "Augmentation pair: ('gaussian_noise', 'color_distortion')\n",
            "Augmentation pair: ('gaussian_noise', 'gaussian_blur')\n",
            "Augmentation pair: ('gaussian_noise', 'cutout')\n",
            "Augmentation pair: ('gaussian_noise', 'sobel_filter')\n",
            "Augmentation pair: ('gaussian_noise', 'gaussian_noise')\n",
            "Augmentation pair: ('gaussian_noise', 'rotate')\n",
            "Augmentation pair: ('rotate', 'random_crop_and_resize')\n",
            "Augmentation pair: ('rotate', 'horizontal_flip')\n",
            "Augmentation pair: ('rotate', 'color_distortion')\n",
            "Augmentation pair: ('rotate', 'gaussian_blur')\n",
            "Augmentation pair: ('rotate', 'cutout')\n",
            "Augmentation pair: ('rotate', 'sobel_filter')\n",
            "Augmentation pair: ('rotate', 'gaussian_noise')\n",
            "Augmentation pair: ('rotate', 'rotate')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results_table = []\n",
        "test_results_table = []\n",
        "\n",
        "for aug1 in augmentations:\n",
        "\n",
        "    train_resulsts = []\n",
        "    test_results = []\n",
        "\n",
        "    for aug2 in augmentations:\n",
        "        train_resulsts.append(results[(aug1, aug2)][0])\n",
        "        test_results.append(results[(aug1, aug2)][1])\n",
        "\n",
        "    train_results_table.append(train_resulsts)\n",
        "    test_results_table.append(test_results)"
      ],
      "metadata": {
        "id": "8jk1j-x2TvO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating sns heatmap and save\n",
        "plt.clf()\n",
        "sns.set(font_scale=1)\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "sns.heatmap(train_results_table, annot=True, cmap='Blues', ax=ax)\n",
        "plt.xticks(np.arange(len(augmentations)) + 0.25, augmentations, rotation=45)\n",
        "plt.yticks(np.arange(len(augmentations)) + 0.25, augmentations, rotation=0)\n",
        "plt.xlabel('Augmentation 2')\n",
        "plt.ylabel('Augmentation 1')\n",
        "plt.title('Train Accuracy Heatmap')\n",
        "plt.savefig('figures/train_accuracy_heatmap.png')"
      ],
      "metadata": {
        "id": "gVM2eOq9GkQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "fig, ax = plt.subplots(figsize=(20, 20))\n",
        "sns.heatmap(test_results_table, annot=True, cmap='Blues', ax=ax)\n",
        "plt.xticks(np.arange(len(augmentations)) + 0.25, augmentations, rotation=45)\n",
        "plt.yticks(np.arange(len(augmentations)) + 0.25, augmentations, rotation=0)\n",
        "plt.xlabel('Augmentation 2')\n",
        "plt.ylabel('Augmentation 1')\n",
        "plt.title('Test Accuracy Heatmap')\n",
        "plt.savefig('figures/test_accuracy_heatmap.png')"
      ],
      "metadata": {
        "id": "AqC2bw8rY3se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: Comparing linear vs non-linear projection heads"
      ],
      "metadata": {
        "id": "t6vMOCbsLhLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######## Pretraining ResNet-50 with Non-linear and Linear Projection Heads ########\n",
        "\n",
        "# define the necessary variables\n",
        "epochs = 15\n",
        "temperature = 0.5\n",
        "base_learning_rate = 0.1 * batch_size / 256\n",
        "warmup_epochs = 0\n",
        "n_classes = 6\n",
        "hidden_dims = [2**i for i in range(5, 12)]\n",
        "\n",
        "# Create the learning rate schedule\n",
        "lr_schedule = MyLRSchedule(base_learning_rate, warmup_epochs * train_dataset_count, epochs * train_dataset_count)\n",
        "\n",
        "linear_head_accuracies = []\n",
        "nonlinear_head_accuracies = []\n",
        "\n",
        "for hidden_dim in hidden_dims:\n",
        "\n",
        "    for model_type in ['nonlinear', 'linear']:\n",
        "\n",
        "        ##### training part #####\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True, weight_decay=None,)\n",
        "\n",
        "        backbone = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224, 224, 3), pooling='avg')\n",
        "        backbone.trainable = True\n",
        "\n",
        "        # create the approporiate projection head\n",
        "        projection_head = tf.keras.models.Sequential()\n",
        "        if model_type == 'nonlinear':\n",
        "            projection_head.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "        projection_head.add(tf.keras.layers.Dense(hidden_dim))\n",
        "        projection_head.trainable = True\n",
        "\n",
        "        # add projection head and backbone together\n",
        "        inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "        feature_extractor = backbone(inputs)\n",
        "        projection_output = projection_head(feature_extractor)\n",
        "        model = tf.keras.models.Model(inputs=inputs, outputs=projection_output)\n",
        "\n",
        "        # train the model\n",
        "        model, optimizer, losses, top_1_acuracies, top_5_acuracie, test_acc = train(epochs, train_dataset, model, optimizer, temperature, print_acc=False)\n",
        "\n",
        "        ##### evaluation part #####\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=base_learning_rate,momentum=0.9,nesterov=True,weight_decay=None,)\n",
        "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        model.trainable = False\n",
        "        linear_classifier = tf.keras.models.Sequential([tf.keras.layers.Dense(n_classes, activation=None)])\n",
        "        linear_classifier.trainable = True\n",
        "\n",
        "        inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "        feature_extractor = model(inputs)\n",
        "        linear_classifier_output = linear_classifier(feature_extractor)\n",
        "        evaluation_model = tf.keras.models.Model(inputs=inputs, outputs=linear_classifier_output)\n",
        "\n",
        "        ## compile and run model\n",
        "        evaluation_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "        evaluation_model.fit(train_dataset, epochs=epochs, verbose=0)\n",
        "\n",
        "        if model_type == 'nonlinear':\n",
        "            nonlinear_head_accuracies.append([evaluation_model.history.history['accuracy'][-1], evaluation_model.evaluate(test_dataset, verbose=0)[1]])\n",
        "        else:\n",
        "            linear_head_accuracies.append([evaluation_model.history.history['accuracy'][-1], evaluation_model.evaluate(test_dataset, verbose=0)[1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFWY7yiQgCrI",
        "outputId": "6800483e-83a0-4462-be12-f419ef12da2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 2.8488\n",
            "Epoch 2, Average Loss: 2.9312\n",
            "Epoch 3, Average Loss: 3.0591\n",
            "Epoch 4, Average Loss: 3.0616\n",
            "Epoch 5, Average Loss: 3.0618\n",
            "Epoch 6, Average Loss: 3.0619\n",
            "Epoch 7, Average Loss: 3.0617\n",
            "Epoch 8, Average Loss: 3.0617\n",
            "Epoch 9, Average Loss: 3.0618\n",
            "Epoch 10, Average Loss: 3.0618\n",
            "Epoch 11, Average Loss: 3.0616\n",
            "Epoch 12, Average Loss: 3.0618\n",
            "Epoch 13, Average Loss: 3.0616\n",
            "Epoch 14, Average Loss: 3.0619\n",
            "Epoch 15, Average Loss: 3.0616\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 2.8365\n",
            "Epoch 2, Average Loss: 3.0538\n",
            "Epoch 3, Average Loss: 3.0604\n",
            "Epoch 4, Average Loss: 3.0589\n",
            "Epoch 5, Average Loss: 3.0574\n",
            "Epoch 6, Average Loss: 3.0609\n",
            "Epoch 7, Average Loss: 3.0607\n",
            "Epoch 8, Average Loss: 3.0607\n",
            "Epoch 9, Average Loss: 3.0595\n",
            "Epoch 10, Average Loss: 3.0512\n",
            "Epoch 11, Average Loss: 3.0404\n",
            "Epoch 12, Average Loss: 3.0615\n",
            "Epoch 13, Average Loss: 3.0616\n",
            "Epoch 14, Average Loss: 3.0613\n",
            "Epoch 15, Average Loss: 3.0615\n",
            "Epoch 1, Average Loss: 2.7621\n",
            "Epoch 2, Average Loss: 2.8443\n",
            "Epoch 3, Average Loss: 3.0527\n",
            "Epoch 4, Average Loss: 3.0593\n",
            "Epoch 5, Average Loss: 3.0578\n",
            "Epoch 6, Average Loss: 3.0184\n",
            "Epoch 7, Average Loss: 3.0619\n",
            "Epoch 8, Average Loss: 3.0620\n",
            "Epoch 9, Average Loss: 3.0620\n",
            "Epoch 10, Average Loss: 3.0620\n",
            "Epoch 11, Average Loss: 3.0620\n",
            "Epoch 12, Average Loss: 3.0620\n",
            "Epoch 13, Average Loss: 3.0620\n",
            "Epoch 14, Average Loss: 3.0620\n",
            "Epoch 15, Average Loss: 3.0620\n",
            "Epoch 1, Average Loss: 2.7609\n",
            "Epoch 2, Average Loss: 2.7867\n",
            "Epoch 3, Average Loss: 2.7993\n",
            "Epoch 4, Average Loss: 3.0517\n",
            "Epoch 5, Average Loss: 3.0618\n",
            "Epoch 6, Average Loss: 3.0619\n",
            "Epoch 7, Average Loss: 3.0619\n",
            "Epoch 8, Average Loss: 3.0618\n",
            "Epoch 9, Average Loss: 3.0618\n",
            "Epoch 10, Average Loss: 3.0618\n",
            "Epoch 11, Average Loss: 3.0619\n",
            "Epoch 12, Average Loss: 3.0617\n",
            "Epoch 13, Average Loss: 3.0617\n",
            "Epoch 14, Average Loss: 3.0618\n",
            "Epoch 15, Average Loss: 3.0616\n",
            "Epoch 1, Average Loss: 2.6608\n",
            "Epoch 2, Average Loss: 2.7865\n",
            "Epoch 3, Average Loss: 2.9277\n",
            "Epoch 4, Average Loss: 2.8074\n",
            "Epoch 5, Average Loss: 2.9079\n",
            "Epoch 6, Average Loss: 2.8929\n",
            "Epoch 7, Average Loss: 3.0461\n",
            "Epoch 8, Average Loss: 3.0377\n",
            "Epoch 9, Average Loss: 3.0313\n",
            "Epoch 10, Average Loss: 2.9542\n",
            "Epoch 11, Average Loss: 2.7354\n",
            "Epoch 12, Average Loss: 2.8076\n",
            "Epoch 13, Average Loss: 3.0233\n",
            "Epoch 14, Average Loss: 3.0773\n",
            "Epoch 15, Average Loss: 3.1343\n",
            "Epoch 1, Average Loss: 2.7185\n",
            "Epoch 2, Average Loss: 2.7845\n",
            "Epoch 3, Average Loss: 2.9389\n",
            "Epoch 4, Average Loss: 2.9610\n",
            "Epoch 5, Average Loss: 3.0436\n",
            "Epoch 6, Average Loss: 3.0547\n",
            "Epoch 7, Average Loss: 3.0022\n",
            "Epoch 8, Average Loss: 2.7829\n",
            "Epoch 9, Average Loss: 3.0605\n",
            "Epoch 10, Average Loss: 3.0611\n",
            "Epoch 11, Average Loss: 3.0599\n",
            "Epoch 12, Average Loss: 3.0555\n",
            "Epoch 13, Average Loss: 3.0330\n",
            "Epoch 14, Average Loss: 2.9521\n",
            "Epoch 15, Average Loss: 2.7971\n",
            "Epoch 1, Average Loss: 2.7281\n",
            "Epoch 2, Average Loss: 2.9230\n",
            "Epoch 3, Average Loss: 2.8440\n",
            "Epoch 4, Average Loss: 2.9072\n",
            "Epoch 5, Average Loss: 2.7019\n",
            "Epoch 6, Average Loss: 2.8275\n",
            "Epoch 7, Average Loss: 2.7069\n",
            "Epoch 8, Average Loss: 2.5356\n",
            "Epoch 9, Average Loss: 2.5029\n",
            "Epoch 10, Average Loss: 2.4974\n",
            "Epoch 11, Average Loss: 2.3829\n",
            "Epoch 12, Average Loss: 2.5612\n",
            "Epoch 13, Average Loss: 2.5362\n",
            "Epoch 14, Average Loss: 2.4984\n",
            "Epoch 15, Average Loss: 2.1635\n",
            "Epoch 1, Average Loss: 2.7043\n",
            "Epoch 2, Average Loss: 2.7174\n",
            "Epoch 3, Average Loss: 2.7700\n",
            "Epoch 4, Average Loss: 2.6839\n",
            "Epoch 5, Average Loss: 2.6382\n",
            "Epoch 6, Average Loss: 2.5003\n",
            "Epoch 7, Average Loss: 2.4756\n",
            "Epoch 8, Average Loss: 2.5553\n",
            "Epoch 9, Average Loss: 2.5400\n",
            "Epoch 10, Average Loss: 2.4546\n",
            "Epoch 11, Average Loss: 2.4840\n",
            "Epoch 12, Average Loss: 2.4037\n",
            "Epoch 13, Average Loss: 2.5519\n",
            "Epoch 14, Average Loss: 2.4211\n",
            "Epoch 15, Average Loss: 2.2559\n",
            "Epoch 1, Average Loss: 2.8333\n",
            "Epoch 2, Average Loss: 2.9416\n",
            "Epoch 3, Average Loss: 2.8838\n",
            "Epoch 4, Average Loss: 2.8597\n",
            "Epoch 5, Average Loss: 2.8707\n",
            "Epoch 6, Average Loss: 2.8107\n",
            "Epoch 7, Average Loss: 2.8236\n",
            "Epoch 8, Average Loss: 2.6396\n",
            "Epoch 9, Average Loss: 2.6792\n",
            "Epoch 10, Average Loss: 2.7858\n",
            "Epoch 11, Average Loss: 2.5685\n",
            "Epoch 12, Average Loss: 2.8398\n",
            "Epoch 13, Average Loss: 2.7173\n",
            "Epoch 14, Average Loss: 2.6758\n",
            "Epoch 15, Average Loss: 2.5872\n",
            "Epoch 1, Average Loss: 2.7263\n",
            "Epoch 2, Average Loss: 2.7190\n",
            "Epoch 3, Average Loss: 2.6440\n",
            "Epoch 4, Average Loss: 2.6531\n",
            "Epoch 5, Average Loss: 2.6717\n",
            "Epoch 6, Average Loss: 2.6098\n",
            "Epoch 7, Average Loss: 2.5297\n",
            "Epoch 8, Average Loss: 2.4714\n",
            "Epoch 9, Average Loss: 2.5050\n",
            "Epoch 10, Average Loss: 2.4098\n",
            "Epoch 11, Average Loss: 2.2639\n",
            "Epoch 12, Average Loss: 2.2538\n",
            "Epoch 13, Average Loss: 2.3351\n",
            "Epoch 14, Average Loss: 2.2350\n",
            "Epoch 15, Average Loss: 2.2297\n",
            "Epoch 1, Average Loss: 2.8019\n",
            "Epoch 2, Average Loss: 2.9009\n",
            "Epoch 3, Average Loss: 2.7287\n",
            "Epoch 4, Average Loss: 2.8319\n",
            "Epoch 5, Average Loss: 2.6078\n",
            "Epoch 6, Average Loss: 2.6344\n",
            "Epoch 7, Average Loss: 2.6209\n",
            "Epoch 8, Average Loss: 2.6645\n",
            "Epoch 9, Average Loss: 2.4317\n",
            "Epoch 10, Average Loss: 2.5342\n",
            "Epoch 11, Average Loss: 2.4323\n",
            "Epoch 12, Average Loss: 2.4421\n",
            "Epoch 13, Average Loss: 2.4339\n",
            "Epoch 14, Average Loss: 2.5100\n",
            "Epoch 15, Average Loss: 2.3181\n",
            "Epoch 1, Average Loss: 2.7457\n",
            "Epoch 2, Average Loss: 2.5499\n",
            "Epoch 3, Average Loss: 2.5274\n",
            "Epoch 4, Average Loss: 2.5075\n",
            "Epoch 5, Average Loss: 2.6845\n",
            "Epoch 6, Average Loss: 2.6691\n",
            "Epoch 7, Average Loss: 2.5820\n",
            "Epoch 8, Average Loss: 2.4284\n",
            "Epoch 9, Average Loss: 2.4687\n",
            "Epoch 10, Average Loss: 2.6136\n",
            "Epoch 11, Average Loss: 2.6887\n",
            "Epoch 12, Average Loss: 2.5662\n",
            "Epoch 13, Average Loss: 2.5702\n",
            "Epoch 14, Average Loss: 2.5455\n",
            "Epoch 15, Average Loss: 2.5230\n",
            "Epoch 1, Average Loss: 2.7145\n",
            "Epoch 2, Average Loss: 2.8832\n",
            "Epoch 3, Average Loss: 3.0020\n",
            "Epoch 4, Average Loss: 3.0612\n",
            "Epoch 5, Average Loss: 3.0616\n",
            "Epoch 6, Average Loss: 3.0617\n",
            "Epoch 7, Average Loss: 3.0613\n",
            "Epoch 8, Average Loss: 3.0615\n",
            "Epoch 9, Average Loss: 3.0617\n",
            "Epoch 10, Average Loss: 3.0619\n",
            "Epoch 11, Average Loss: 3.0616\n",
            "Epoch 12, Average Loss: 3.0617\n",
            "Epoch 13, Average Loss: 3.0617\n",
            "Epoch 14, Average Loss: 3.0615\n",
            "Epoch 15, Average Loss: 3.0614\n",
            "Epoch 1, Average Loss: 2.6962\n",
            "Epoch 2, Average Loss: 2.6579\n",
            "Epoch 3, Average Loss: 2.6607\n",
            "Epoch 4, Average Loss: 2.6258\n",
            "Epoch 5, Average Loss: 2.5751\n",
            "Epoch 6, Average Loss: 2.3614\n",
            "Epoch 7, Average Loss: 2.4583\n",
            "Epoch 8, Average Loss: 2.4620\n",
            "Epoch 9, Average Loss: 2.3952\n",
            "Epoch 10, Average Loss: 2.3604\n",
            "Epoch 11, Average Loss: 2.3847\n",
            "Epoch 12, Average Loss: 2.3808\n",
            "Epoch 13, Average Loss: 2.2627\n",
            "Epoch 14, Average Loss: 2.2548\n",
            "Epoch 15, Average Loss: 2.2942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## Pretraining ResNet-50 for Comparison ########\n",
        "\n",
        "# define the necessary variables\n",
        "epochs = 15\n",
        "temperature = 0.5\n",
        "base_learning_rate = 0.1 * batch_size / 256\n",
        "warmup_epochs = 0\n",
        "n_classes = 6\n",
        "\n",
        "##### training part #####\n",
        "# Training model with no projection head for comparison\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True, weight_decay=None,)\n",
        "backbone = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224, 224, 3), pooling='avg')\n",
        "backbone.trainable = True\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "outputs = backbone(inputs)\n",
        "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model, optimizer, losses, top_1_acuracies, top_5_acuracie, test_acc = train(epochs, train_dataset, model, optimizer, temperature, print_acc=False)\n",
        "\n",
        "##### evaluation part #####\n",
        "model.trainable = False\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=base_learning_rate,momentum=0.9,nesterov=True,weight_decay=None,)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "backbone = tf.keras.applications.ResNet50(include_top=False, weights='imagenet', input_tensor=None, input_shape=(224, 224, 3), pooling='avg')\n",
        "backbone.trainable = True\n",
        "\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "outputs = backbone(inputs)\n",
        "evaluation_model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "evaluation_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "evaluation_model.fit(train_dataset, epochs=epochs, verbose=0)\n",
        "\n",
        "train_acc_no_projection = evaluation_model.history.history['accuracy'][-1]\n",
        "test_acc_no_projection = evaluation_model.evaluate(test_dataset, verbose=0)[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlaCDbNnycpF",
        "outputId": "2e97ca28-a27b-427a-c2ef-5c8d2dbdef2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 2.6435\n",
            "Epoch 2, Average Loss: 2.7298\n",
            "Epoch 3, Average Loss: 2.7555\n",
            "Epoch 4, Average Loss: 2.7597\n",
            "Epoch 5, Average Loss: 3.0508\n",
            "Epoch 6, Average Loss: 3.0594\n",
            "Epoch 7, Average Loss: 3.0600\n",
            "Epoch 8, Average Loss: 3.0601\n",
            "Epoch 9, Average Loss: 3.0569\n",
            "Epoch 10, Average Loss: 3.0570\n",
            "Epoch 11, Average Loss: 3.0430\n",
            "Epoch 12, Average Loss: 3.0349\n",
            "Epoch 13, Average Loss: 2.9797\n",
            "Epoch 14, Average Loss: 2.9447\n",
            "Epoch 15, Average Loss: 2.9388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bar_width = 0.35\n",
        "positions = [i for i in range(len(hidden_dims))]\n",
        "positions_linear = [p - bar_width / 2 for p in positions]\n",
        "positions_nonlinear = [p + bar_width / 2 for p in positions]\n",
        "\n",
        "plt.clf()\n",
        "plt.bar(positions_linear, [e[0] for e in linear_head_accuracies], width=bar_width, label='Linear Head')\n",
        "plt.bar(positions_nonlinear, [e[0] for e in nonlinear_head_accuracies], width=bar_width, label='Nonelinear Head')\n",
        "plt.bar(positions_nonlinear[-1] + bar_width, train_acc_no_projection, width=bar_width, label='No Projection Head')\n",
        "plt.legend()\n",
        "plt.xticks(positions, hidden_dims)\n",
        "plt.xlabel('Hidden Dimension')\n",
        "plt.ylabel('Top-1 Accuracy')\n",
        "plt.title('Top-1 Train Accuracy vs Hidden Dimension ')\n",
        "plt.savefig('figures/bar_projection_heads_accuracies_train.png')"
      ],
      "metadata": {
        "id": "I4__qkr9mq8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bar_width = 0.35\n",
        "positions = [i for i in range(len(hidden_dims))]\n",
        "positions_linear = [p - bar_width / 2 for p in positions]\n",
        "positions_nonlinear = [p + bar_width / 2 for p in positions]\n",
        "\n",
        "plt.clf()\n",
        "plt.bar(positions_linear, [e[1] for e in linear_head_accuracies], width=bar_width, label='Linear Head')\n",
        "plt.bar(positions_nonlinear, [e[1] for e in nonlinear_head_accuracies], width=bar_width, label='Nonelinear Head')\n",
        "plt.bar(positions_nonlinear[-1] + bar_width, test_acc_no_projection, width=bar_width, label='No Projection Head')\n",
        "plt.legend()\n",
        "plt.xticks(positions, hidden_dims)\n",
        "plt.xlabel('Hidden Dimension')\n",
        "plt.ylabel('Top-1 Accuracy')\n",
        "plt.title('Top-1 Test Accuracy vs Hidden Dimension')\n",
        "plt.savefig('figures/bar_projection_heads_accuracies_test.png')"
      ],
      "metadata": {
        "id": "PlwYyPKYsvJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = ['random_crop_and_resize', 'horizontal_flip', 'color_distortion', 'gaussian_blur', 'cutout', 'sobel_filter', 'gaussian_noise', 'rotate']\n",
        "\n",
        "for images, label in train_dataset.take(1):\n",
        "    for img in images:\n",
        "        X_augmented_1 = augment_image(img, target_size=(224, 224), target_augmentations=augmentations)\n",
        "        plt.imshow(X_augmented_1 / 255)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "GzLwvc_ofElU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRGVgDrTRvwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "def print_tree(directory, level=0):\n",
        "    path = pathlib.Path(directory)\n",
        "    for item in path.iterdir():\n",
        "        print(\"|   \" * level + \"+-- \" + item.name)\n",
        "        if item.is_dir():\n",
        "            print_tree(item, level + 1)\n",
        "\n",
        "# Example usage\n",
        "print_tree('.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fas2_2k4QUNZ",
        "outputId": "23e27941-e3f0-4860-d85f-30a7f6ec6017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-- .git\n",
            "|   +-- branches\n",
            "|   +-- info\n",
            "|   |   +-- exclude\n",
            "|   +-- hooks\n",
            "|   |   +-- pre-merge-commit.sample\n",
            "|   |   +-- pre-commit.sample\n",
            "|   |   +-- prepare-commit-msg.sample\n",
            "|   |   +-- push-to-checkout.sample\n",
            "|   |   +-- pre-applypatch.sample\n",
            "|   |   +-- applypatch-msg.sample\n",
            "|   |   +-- fsmonitor-watchman.sample\n",
            "|   |   +-- pre-receive.sample\n",
            "|   |   +-- post-update.sample\n",
            "|   |   +-- pre-rebase.sample\n",
            "|   |   +-- pre-push.sample\n",
            "|   |   +-- commit-msg.sample\n",
            "|   |   +-- update.sample\n",
            "|   +-- description\n",
            "|   +-- refs\n",
            "|   |   +-- heads\n",
            "|   |   |   +-- main\n",
            "|   |   +-- tags\n",
            "|   |   +-- remotes\n",
            "|   |   |   +-- origin\n",
            "|   |   |   |   +-- HEAD\n",
            "|   +-- objects\n",
            "|   |   +-- pack\n",
            "|   |   |   +-- pack-b896c96a6807667d984bca8e6ced78c99265ce87.pack\n",
            "|   |   |   +-- pack-b896c96a6807667d984bca8e6ced78c99265ce87.idx\n",
            "|   |   +-- info\n",
            "|   +-- config\n",
            "|   +-- packed-refs\n",
            "|   +-- logs\n",
            "|   |   +-- refs\n",
            "|   |   |   +-- remotes\n",
            "|   |   |   |   +-- origin\n",
            "|   |   |   |   |   +-- HEAD\n",
            "|   |   |   +-- heads\n",
            "|   |   |   |   +-- main\n",
            "|   |   +-- HEAD\n",
            "|   +-- HEAD\n",
            "|   +-- index\n",
            "+-- E4040.2024Fall.SAMH.report.sh4635.pdf\n",
            "+-- README.md\n",
            "+-- SimCLR.ipynb\n",
            "+-- figures\n",
            "|   +-- bar_projection_heads_accuracies_test.png\n",
            "|   +-- bar_projection_heads_accuracies_train.png\n",
            "|   +-- linear_classifier_sample_running_accuracy.png\n",
            "|   +-- linear_classifier_sample_running_loss.png\n",
            "|   +-- linear_pretraining_loss.png\n",
            "|   +-- loss_comparison.png\n",
            "|   +-- nonlinear_pretraining_loss.png\n",
            "|   +-- test_accuracy_heatmap.png\n",
            "|   +-- train_accuracy_heatmap.png\n",
            "+-- requirements.txt\n",
            "+-- training\n",
            "|   +-- learning_rate_schedule.py\n",
            "|   +-- train.py\n",
            "+-- utils\n",
            "|   +-- NT_Xent.py\n",
            "|   +-- data_augmentations.py\n",
            "|   +-- evaluation_metrics.py\n",
            "|   +-- test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_RH_AMKLRXnr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "N8zUcvHv-m3z",
        "ZNLBV90g_KXt",
        "pi_sFfZKTCLJ",
        "SHeNDqw0TEnR",
        "0pT-fJ7GTG3o",
        "TkNRYX6kB0SZ",
        "9o1pl6ZQgm3_",
        "7SuVZ1Yc82pY",
        "t6vMOCbsLhLn",
        "IARKibA2s8cQ",
        "mGu9qCaugsKu"
      ],
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}